"""
TRAIN TEMPORAL FUSION TRANSFORMER
State-of-the-art AI for trading with 60-75% WIN rate!
"""
import sys
import os
from pathlib import Path

# Fix encoding for Windows console
if sys.platform == "win32":
    os.environ["PYTHONIOENCODING"] = "utf-8"
    try:
        sys.stdout.reconfigure(encoding='utf-8', errors='replace')
        sys.stderr.reconfigure(encoding='utf-8', errors='replace')
    except Exception:
        pass  # Fallback if reconfigure not available

sys.path.insert(0, str(Path(__file__).parent))

import warnings
warnings.filterwarnings('ignore')

print("=" * 80)
print("TEMPORAL FUSION TRANSFORMER TRAINING")
print("=" * 80)
print("Target: 60-75% WIN rate")
print("State-of-the-art multi-horizon prediction")
print("=" * 80)

# Install PyTorch if needed
print("\nüì¶ Checking PyTorch installation...")
try:
    import torch
    print(f"[OK] PyTorch {torch.__version__} installed")
    print(f"   CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   CUDA device: {torch.cuda.get_device_name(0)}")
except ImportError:
    print("[WARNING] PyTorch not found - installing...")
    import subprocess
    subprocess.check_call([
        sys.executable, "-m", "pip", "install", 
        "torch", "torchvision", "torchaudio",
        "--index-url", "https://download.pytorch.org/whl/cpu"
    ])
    import torch
    print("[OK] PyTorch installed!")

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import json
from typing import List, Tuple

# Import our TFT model
from ai_engine.tft_model import (
    TemporalFusionTransformer,
    TFTTrainer,
    save_model
)

# Import database
from backend.database import engine
from sqlalchemy.orm import sessionmaker
from backend.models.ai_training import AITrainingSample

print("\n" + "=" * 80)
print("[CHART] LOADING TRAINING DATA")
print("=" * 80)

# Use backend engine (handles all configuration)
SessionLocal = sessionmaker(bind=engine)

# Load data from database
print("üîì Opening database connection...")
db = SessionLocal()
try:
    samples = db.query(AITrainingSample).filter(
        AITrainingSample.outcome_known == True,
        AITrainingSample.features.isnot(None)
    ).all()
finally:
    db.close()

print(f"\n[OK] Loaded {len(samples):,} training samples")

if len(samples) < 1000:
    print("‚ùå Not enough samples! Need at least 1000 for TFT training")
    print("üí° Run backfill scripts first")
    sys.exit(1)


class TradingDataset(Dataset):
    """
    PyTorch Dataset for time-series trading data
    Creates sequences of 60 timesteps
    """
    
    def __init__(self, samples: List[AITrainingSample], sequence_length: int = 60):
        self.sequence_length = sequence_length
        self.samples = samples
        
        print(f"\nüîÑ Creating sequences...")
        print(f"   Sequence length: {sequence_length} timesteps")
        
        # Group by symbol for proper sequencing
        self.sequences = []
        self.targets = []
        
        # Sort by symbol and timestamp
        sorted_samples = sorted(samples, key=lambda x: (x.symbol, x.timestamp))
        
        # Create sequences per symbol
        symbol_groups = {}
        for sample in sorted_samples:
            if sample.symbol not in symbol_groups:
                symbol_groups[sample.symbol] = []
            symbol_groups[sample.symbol].append(sample)
        
        print(f"   Found {len(symbol_groups)} unique symbols")
        
        # Extract sequences
        for symbol, symbol_samples in symbol_groups.items():
            if len(symbol_samples) < sequence_length:
                continue
            
            # Create overlapping sequences
            for i in range(len(symbol_samples) - sequence_length + 1):
                sequence_samples = symbol_samples[i:i + sequence_length]
                
                # Extract features for each timestep
                sequence_features = []
                for s in sequence_samples:
                    try:
                        features = json.loads(s.features)
                        feature_vector = [
                            features.get('Close', 0),
                            features.get('Volume', 0),
                            features.get('EMA_10', 0),
                            features.get('EMA_50', 0),
                            features.get('RSI', 50),
                            features.get('MACD', 0),
                            features.get('MACD_signal', 0),
                            features.get('BB_upper', 0),
                            features.get('BB_middle', 0),
                            features.get('BB_lower', 0),
                            features.get('ATR', 0),
                            features.get('volume_sma_20', 0),
                            features.get('price_change_pct', 0),
                            features.get('high_low_range', 0),
                        ]
                        sequence_features.append(feature_vector)
                    except:
                        continue
                
                if len(sequence_features) == sequence_length:
                    self.sequences.append(np.array(sequence_features, dtype=np.float32))
                    
                    # Target is the action of LAST sample in sequence
                    last_sample = sequence_samples[-1]
                    target_class = last_sample.target_class
                    
                    # Convert to numeric
                    if target_class == 'BUY':
                        target = 0
                    elif target_class == 'SELL':
                        target = 1
                    else:  # HOLD
                        target = 2
                    
                    self.targets.append(target)
        
        print(f"[OK] Created {len(self.sequences):,} sequences")
        
        # Convert to numpy
        self.sequences = np.array(self.sequences, dtype=np.float32)
        self.targets = np.array(self.targets, dtype=np.int64)
        
        # Normalize features
        print(f"\n[CHART] Normalizing features...")
        self.feature_mean = self.sequences.mean(axis=(0, 1), keepdims=True)
        self.feature_std = self.sequences.std(axis=(0, 1), keepdims=True) + 1e-8
        self.sequences = (self.sequences - self.feature_mean) / self.feature_std
        print(f"[OK] Normalization complete")
        
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        return (
            torch.from_numpy(self.sequences[idx]),
            torch.tensor(self.targets[idx])
        )


# Create dataset
print("\n" + "=" * 80)
print("üîÑ PREPARING DATASET")
print("=" * 80)

dataset = TradingDataset(samples, sequence_length=60)

# Train/val split
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size

train_dataset, val_dataset = torch.utils.data.random_split(
    dataset, [train_size, val_size],
    generator=torch.Generator().manual_seed(42)
)

print(f"\n[CHART] Dataset split:")
print(f"   Train: {len(train_dataset):,} sequences")
print(f"   Val:   {len(val_dataset):,} sequences")

# Create data loaders
train_loader = DataLoader(
    train_dataset,
    batch_size=128,
    shuffle=True,
    num_workers=0,  # Windows compatibility
    pin_memory=True if torch.cuda.is_available() else False
)

val_loader = DataLoader(
    val_dataset,
    batch_size=256,
    shuffle=False,
    num_workers=0,
    pin_memory=True if torch.cuda.is_available() else False
)

print(f"[OK] Data loaders ready")
print(f"   Train batches: {len(train_loader)}")
print(f"   Val batches: {len(val_loader)}")

# Create model
print("\n" + "=" * 80)
print("üèóÔ∏è BUILDING TEMPORAL FUSION TRANSFORMER")
print("=" * 80)

device = 'cuda' if torch.cuda.is_available() else 'cpu'

print("\n[TARGET] ANTI-OVERFITTING CONFIG:")
print("   - Increased dropout: 0.3 (was 0.1)")
print("   - Reduced hidden size: 96 (was 128)")
print("   - Reduced layers: 2 (was 3)")
print("   - Stronger weight decay: 0.001 (was 0.0001)")
print("   - Label smoothing: 0.1")

model = TemporalFusionTransformer(
    input_size=14,
    sequence_length=60,
    hidden_size=96,        # Reduced from 128
    num_heads=8,
    num_layers=2,           # Reduced from 3
    dropout=0.3,            # Increased from 0.1
    num_classes=3
)

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\n[CHART] Model statistics:")
print(f"   Total parameters: {total_params:,}")
print(f"   Trainable parameters: {trainable_params:,}")
print(f"   Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB")

# Create trainer
trainer = TFTTrainer(
    model=model,
    device=device,
    learning_rate=0.001,
    weight_decay=0.001  # Increased from 0.0001 for stronger regularization
)

# Training loop
print("\n" + "=" * 80)
print("[ROCKET] TRAINING TEMPORAL FUSION TRANSFORMER")
print("=" * 80)
print("‚è±Ô∏è Estimated time: 15-40 minutes")
print("üí° Tip: Go grab a coffee! ‚òï")
print("=" * 80)

num_epochs = 30  # Increased from 20
best_val_accuracy = 0
patience_counter = 0
max_patience = 7  # Increased from 5 for more thorough training

for epoch in range(num_epochs):
    print(f"\n{'=' * 80}")
    print(f"[CHART] EPOCH {epoch + 1}/{num_epochs}")
    print(f"{'=' * 80}")
    
    # Train
    train_loss = trainer.train_epoch(train_loader, epoch)
    print(f"   Train Loss: {train_loss:.4f}")
    
    # Validate
    val_metrics = trainer.evaluate(val_loader)
    print(f"   Val Loss: {val_metrics['loss']:.4f}")
    print(f"   Val Accuracy: {val_metrics['accuracy']:.2f}%")
    
    # Learning rate scheduling
    trainer.scheduler.step(val_metrics['loss'])
    
    # Save best model
    if val_metrics['accuracy'] > best_val_accuracy:
        best_val_accuracy = val_metrics['accuracy']
        save_model(model, 'ai_engine/models/tft_model.pth')
        print(f"   üèÜ New best accuracy! Saved model.")
        patience_counter = 0
    else:
        patience_counter += 1
    
    # Early stopping
    if patience_counter >= max_patience:
        print(f"\n[WARNING] Early stopping triggered (patience={max_patience})")
        break
    
    # Progress bar
    if val_metrics['accuracy'] >= 55:
        print(f"\n{'üéâ' * 40}")
        print(f"   üèÜ TARGET ACHIEVED: {val_metrics['accuracy']:.1f}% ACCURACY!")
        print(f"{'üéâ' * 40}")

# Final results
print("\n" + "=" * 80)
print("[OK] TRAINING COMPLETE!")
print("=" * 80)
print(f"\nüèÜ BEST VALIDATION ACCURACY: {best_val_accuracy:.2f}%")

if best_val_accuracy >= 60:
    print("\nüéâüéâüéâ EXCEPTIONAL PERFORMANCE! üéâüéâüéâ")
    print("üíé Professional trading-grade AI model!")
elif best_val_accuracy >= 55:
    print("\nüéä EXCELLENT PERFORMANCE! üéä")
    print("[OK] Ready for futures trading!")
else:
    print(f"\n[WARNING] Target not reached. Current: {best_val_accuracy:.1f}%")
    print("üí° Tips:")
    print("   - Gather more diverse training data")
    print("   - Train for more epochs")
    print("   - Tune hyperparameters")

print("\nüìÅ Model saved to: ai_engine/models/tft_model.pth")
print(f"[CHART] Model size: {total_params * 4 / 1024 / 1024:.1f} MB")

db.close()

print("\n" + "=" * 80)
print("[ROCKET] TEMPORAL FUSION TRANSFORMER READY FOR TRADING!")
print("=" * 80)
