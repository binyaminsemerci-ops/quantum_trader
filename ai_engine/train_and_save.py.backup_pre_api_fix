import os
import asyncio
import pickle
import logging
from datetime import datetime, timezone
from typing import List, Optional, Any

"""Simple training harness for the XGBoost agent.

This script fetches data from the internal `backend.routes.external_data` helpers
to build a dataset, adds technical + sentiment features, trains a regressor,
and writes model and scaler artifacts to `ai_engine/models/`.

The script is defensive with multiple fallback levels:
1. XGBRegressor (preferred, when xgboost is available)
2. RandomForestRegressor (fallback, when sklearn is available)
3. DummyRegressor (fallback, basic sklearn estimator)
4. _MeanRegressor (final fallback, minimal implementation)
This ensures training can proceed in minimal CI/dev environments.
"""

MODEL_DIR = os.path.join(os.path.dirname(__file__), "models")
os.makedirs(MODEL_DIR, exist_ok=True)

logger = logging.getLogger(__name__)


class _SimpleScaler:
    """Very small replacement for sklearn StandardScaler when not available."""

    def fit(self, X):
        import numpy as _np

        self.mean_ = _np.nanmean(X, axis=0)
        self.scale_ = _np.nanstd(X, axis=0)
        self.scale_[self.scale_ == 0] = 1.0

    def transform(self, X):

        return (X - self.mean_) / self.scale_

    def fit_transform(self, X):
        self.fit(X)
        return self.transform(X)


class _MeanRegressor:
    """Simple mean predictor used when no regressor is available."""

    def fit(self, X, y):
        import numpy as _np

        self.mean_ = float(_np.nanmean(y))

    def predict(self, X):
        import numpy as _np

        return _np.full((len(X),), getattr(self, "mean_", 0.0))


async def _fetch_symbol_data(symbol: str, limit: int = 600):
    # Import the specific module directly
    import backend.routes.external_data as external_data

    # external_data.binance_ohlcv is async; call with asyncio
    resp = await external_data.binance_ohlcv(symbol=symbol, limit=limit)
    candles = resp.get("candles", [])

    # sentiment: try twitter client (free source)
    tw = await external_data.twitter_sentiment(symbol=symbol)
    sent_score = 0.0
    try:
        sent_score = float(tw.get("score", tw.get("sentiment", {}).get("score", 0.0)))
    except Exception:
        sent_score = 0.0

    # No cryptopanic/news: only use free sources
    n = len(candles)
    sentiment_series = [sent_score] * n
    news_series = [0] * n  # No news events

    return candles, sentiment_series, news_series


def build_dataset(all_symbol_data):
    """Given list of (symbol, candles, sentiment, news) tuples, build X,y arrays.

    This implementation avoids optional helpers and computes a simple forward
    1-step return target directly from Close prices.
    """
    import pandas as pd  # type: ignore[import-untyped]
    import numpy as np
    from ai_engine.feature_engineer import add_technical_indicators, add_sentiment_features  # type: ignore[import-not-found, import-untyped]

    X_list = []
    y_list = []
    for symbol, candles, sentiment, news in all_symbol_data:
        if not candles:
            continue
        df = pd.DataFrame(candles)
        # ensure expected column names
        df = df.rename(columns={c: c.capitalize() for c in df.columns})

        # compute technicals
        try:
            feat = add_technical_indicators(
                df.rename(columns={"Close": "Close", "High": "High", "Low": "Low"})
            )
        except Exception:
            continue

        # add sentiment/news aligned series (optional)
        feat = add_sentiment_features(feat, sentiment_series=sentiment, news_counts=news)

        # compute simple forward 1-step return as target
        if "Close" not in feat.columns:
            continue
        close = pd.to_numeric(feat["Close"], errors="coerce")
        forward_return = (close.shift(-1) / close - 1.0)
        feat = feat.iloc[:-1].copy()
        forward_return = forward_return.iloc[:-1]

        # drop non-numeric columns and rows with NaN
        X_df = feat.select_dtypes(include=[float, int]).copy()
        y_series = forward_return.astype(float)
        mask = (~X_df.isna().any(axis=1)) & (~y_series.isna())
        if mask.sum() == 0:
            continue
        X = X_df[mask].values
        y = y_series[mask].values
        if len(y) == 0 or X.size == 0:
            continue
        X_list.append(X)
        y_list.append(y)

    if not X_list:
        raise RuntimeError("No training data assembled")

    X_all = np.vstack(X_list)
    y_all = np.concatenate(y_list)
    return X_all, y_all


def make_scaler():
    try:
        from sklearn.preprocessing import StandardScaler  # type: ignore[import-untyped]

        return StandardScaler()
    except Exception:
        return _SimpleScaler()


def make_regressor():
    try:
        from xgboost import XGBRegressor  # type: ignore[import-untyped]

        return XGBRegressor(n_estimators=50, max_depth=3, verbosity=0)
    except Exception:
        try:
            from sklearn.ensemble import RandomForestRegressor  # type: ignore[import-untyped]

            return RandomForestRegressor(n_estimators=50, max_depth=5, random_state=42)
        except Exception:
            try:
                from sklearn.dummy import DummyRegressor  # type: ignore[import-untyped]

                return DummyRegressor(strategy="mean")
            except Exception:
                return _MeanRegressor()


def save_artifacts(model, scaler, model_path, scaler_path, *, metadata: dict[str, Any] | None = None):
    with open(model_path, "wb") as f:
        pickle.dump(model, f)
    with open(scaler_path, "wb") as f:
        pickle.dump(scaler, f)
    # write simple metadata JSON next to model
    try:
        import json
        import datetime

        meta = {
            "model_path": model_path,
            "scaler_path": scaler_path,
            "saved_at": datetime.datetime.now(datetime.timezone.utc).isoformat(),
        }
        if metadata:
            meta.update(metadata)
        meta_path = os.path.join(MODEL_DIR, "metadata.json")
        with open(meta_path, "w", encoding="utf-8") as mf:
            json.dump(meta, mf)
    except Exception:
        import logging

        logging.getLogger(__name__).debug(
            "failed to write model metadata", exc_info=True
        )


def train_and_save(symbols: Optional[List[str]] = None, limit: int = 600):
    if symbols is None:
        # prefer USDC as the spot quote by default for training / dataset assembly
        try:
            from config.config import DEFAULT_QUOTE  # type: ignore[import-not-found, import-untyped]

            symbols = [f"BTC{DEFAULT_QUOTE}", f"ETH{DEFAULT_QUOTE}"]
        except Exception:
            symbols = ["BTCUSDC", "ETHUSDC"]

    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    tasks = [_fetch_symbol_data(s, limit=limit) for s in symbols]
    logger.info("Fetching market data for symbols: %s", ", ".join(symbols))
    results = loop.run_until_complete(asyncio.gather(*tasks))
    all_symbol_data = []
    for sym, (candles, sentiment, news) in zip(symbols, results):
        all_symbol_data.append((sym, candles, sentiment, news))

    # Attempt to build dataset using pandas-based routines; if pandas is
    # unavailable (e.g., import-time binary issues), fall back to a
    # synthetic dataset so training can proceed for CI/dev.
    try:
        X, y = build_dataset(all_symbol_data)
        dataset_notes = None
    except Exception as exc:  # pragma: no cover - environment dependent
        logger.warning("build_dataset failed, falling back to synthetic dataset: %s", exc)
        # create a small synthetic dataset
        import numpy as _np

        def synthetic_dataset(samples=1000, features=16):
            rng = _np.random.default_rng(12345)
            Xs = rng.normal(size=(samples, features))
            # create a target correlated with a subset of features
            y = (
                Xs[:, 0] * 0.3
                + Xs[:, 1] * -0.2
                + rng.normal(scale=0.1, size=(samples,))
            )
            return Xs, y

        X, y = synthetic_dataset(samples=1000, features=16)
        dataset_notes = f"synthetic dataset fallback used: {exc}"

    scaler = make_scaler()
    try:
        Xs = scaler.fit_transform(X)
    except Exception:
        try:
            Xs = scaler.fit_transform(X)
        except Exception as e:
            logger.debug("scaler fit_transform failed: %s", e)
            # final fallback: attempt a simple identity-like transform
            Xs = X

    sample_count = int(X.shape[0]) if hasattr(X, "shape") else len(X)
    feature_count = (
        int(X.shape[1]) if hasattr(X, "shape") and len(getattr(X, "shape", ())) > 1 else 0
    )

    reg = make_regressor()
    run_version = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")

    session = None
    run_record = None
    _complete_run = lambda *_args, **_kwargs: None  # noqa: E731
    _fail_run = lambda *_args, **_kwargs: None  # noqa: E731
    try:
        from backend.database import (  # type: ignore[import-error]
            SessionLocal,
            start_model_run as _start_run,
            complete_model_run as _complete_run,
            fail_model_run as _fail_run,
        )

        session = SessionLocal()
        run_record = _start_run(
            session,
            version=run_version,
            symbol_count=len(symbols),
            sample_count=sample_count,
            feature_count=feature_count,
            notes=dataset_notes,
        )
    except Exception as db_exc:  # pragma: no cover - optional dependency path
        logger.debug("Model registry unavailable: %s", db_exc)
        session = None
        run_record = None

    try:
        reg.fit(Xs, y)
    except Exception as train_exc:
        logger.exception("Model training failed")
        if session and run_record:
            try:
                _fail_run(session, run_record.id, metrics={"error": str(train_exc)})
            except Exception:  # pragma: no cover - registry optional
                logger.debug("Failed to record training failure", exc_info=True)
        if session:
            session.close()
        raise

    model_path = os.path.join(MODEL_DIR, "xgb_model.pkl")
    scaler_path = os.path.join(MODEL_DIR, "scaler.pkl")
    metrics = {
        "samples": int(len(y)),
        "features": int(Xs.shape[1] if hasattr(Xs, "shape") and len(Xs.shape) > 1 else feature_count),
    }
    save_artifacts(
        reg,
        scaler,
        model_path,
        scaler_path,
        metadata={
            "version": run_version,
            "samples": metrics["samples"],
            "features": metrics["features"],
            "notes": dataset_notes,
        },
    )

    try:
        if session and run_record:
            _complete_run(
                session,
                run_record.id,
                model_path=model_path,
                scaler_path=scaler_path,
                metrics=metrics,
            )
    except Exception as db_exc:  # pragma: no cover - optional dependency path
        logger.debug("Failed to complete model run record: %s", db_exc)
    finally:
        if session:
            session.close()

    logger.info("Saved model -> %s", model_path)
    logger.info("Saved scaler -> %s", scaler_path)


if __name__ == "__main__":
    # quick local run (will use internal route fallbacks if external APIs are not configured)
    train_and_save()
