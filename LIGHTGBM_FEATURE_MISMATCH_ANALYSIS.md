# ğŸš¨ LightGBM Feature Mismatch - Complete Analysis
**Date**: February 16, 2026 22:40 UTC  
**Issue**: LightGBM model completely broken due to massive feature mismatch

---

## ğŸ“Š THE PROBLEM

### Feature Mismatch Summary

| Component | Expected Features | Actual Features | Gap |
|-----------|------------------|-----------------|-----|
| **LightGBM Model** | **49 features** | 5 provided | **-44 features** |
| **Scaler** | 14 features | 5 provided | -9 features |
| **Feature Stream** | N/A | 17 available | N/A |
| **lgbm_agent.py Code** | 49 needed | 5 extracted | **-44 features** |

---

## ğŸ” DETAILED BREAKDOWN

### Current Feature Stream (17 available):
```
âœ… price
âœ… price_return_1
âœ… price_return_5  
âœ… price_volatility_10
âœ… price_change
âœ… ma_10
âœ… ma_20
âœ… ma_50
âœ… ma_cross_10_20
âœ… rsi_14
âœ… macd
âœ… volume
âœ… volume_ratio
âœ… bb_upper
âœ… bb_lower
âœ… bb_position
âœ… momentum_10
```

### LightGBM Model Requirements (49 features):

**Candlestick Features (10)**:
```
âŒ returns
âŒ log_returns
âŒ price_range
âŒ body_size
âŒ upper_wick
âŒ lower_wick
âŒ is_doji
âŒ is_hammer
âŒ is_engulfing
âŒ gap_up / gap_down
```

**Momentum & Oscillators (13)**:
```
âœ… rsi (as rsi_14 in stream)
âœ… macd
âŒ macd_signal
âŒ macd_hist
âŒ stoch_k
âŒ stoch_d
âŒ roc
âœ… momentum_10
âŒ momentum_5
âŒ momentum_20
âŒ acceleration
âŒ relative_spread
```

**Moving Averages (12)**:
```
âœ… ma_10 / ma_20 / ma_50 (SMA in stream)
âŒ sma_20
âŒ ema_9, ema_9_dist
âŒ ema_21, ema_21_dist
âŒ ema_50, ema_50_dist
âŒ ema_200, ema_200_dist
```

**Bollinger Bands (5)**:
```
âŒ bb_middle
âœ… bb_upper
âœ… bb_lower
âŒ bb_width
âœ… bb_position
```

**Volatility (4)**:
```
âœ… price_volatility_10 (as volatility?)
âŒ atr
âŒ atr_pct
```

**Trend (4)**:
```
âŒ adx
âŒ plus_di
âŒ minus_di
```

**Volume (5)**:
```
âœ… volume
âœ… volume_ratio
âŒ volume_sma
âŒ obv
âŒ obv_ema
âŒ vpt
```

---

## ğŸ¯ ROOT CAUSE

**Model Training vs Production Mismatch**:

1. **Training Phase** (December 2025):
   - Model trained with **49 rich technical features**
   - Includes advanced indicators (ADX, Stochastic, OBV, candlestick patterns)
   - Scaler fitted to 14-feature subset?

2. **Production Deployment** (Now):
   - Feature publisher only generates **17 basic features**
   - lgbm_agent.py code only extracts **5 features**:
     ```python
     feature_names = [
         'price_change',
         'rsi_14',
         'macd',
         'volume_ratio',
         'momentum_10'
     ]
     ```

3. **Result**: 
   - Model expects 49 â†’ Gets 5 â†’ **FAILS EVERY TIME**
   - 6,985 errors/hour (116 errors/minute)
   - System falls back to simple RSI/MACD rules

---

## ğŸ“‹ SOLUTION OPTIONS

### âš¡ **Option A: Disable LightGBM (RECOMMENDED - 15 minutes)**

**Action**:
1. Comment out LightGBM agent in `ensemble_predictor_service.py`
2. Restart ensemble predictor service
3. System uses XGBoost + fallback only

**Pros**:
- âœ… Immediate fix (15 min)
- âœ… Stops error spam
- âœ… Trading continues normally
- âœ… XGBoost may still work

**Cons**:
- âš ï¸ Lose LightGBM predictions (already not working)
- âš ï¸ Slightly degraded ensemble quality

**Code Change**:
```python
# In ensemble_predictor_service.py
agents = {
    "xgboost": xgb_agent,
    # "lgbm": lgbm_agent,  # DISABLED until retrained with correct features
}
```

---

### ğŸ”„ **Option B: Retrain LightGBM with 17 Features (2-4 hours)**

**Action**:
1. Collect training data from Redis/database
2. Extract 17 available features per sample
3. Retrain LightGBM model with smaller feature set
4. Generate new scaler.pkl
5. Deploy model + scaler
6. Restart service

**Pros**:
- âœ… Get ML predictions working again
- âœ… Use existing feature infrastructure
- âœ… Modern model with current features

**Cons**:
- â³ 2-4 hours of work
- â³ Need training data access
- âš ï¸ Lower quality than 49-feature model

---

### ğŸ—ï¸ **Option C: Expand Feature Engineering (4-8 hours)**

**Action**:
1. Update feature publisher to calculate all 49 features
2. Add candlestick pattern detection
3. Add EMAs, ADX, Stochastic, OBV, etc.
4. Test feature generation
5. Deploy updated feature publisher
6. Use existing 49-feature model

**Pros**:
- âœ… Use existing trained model (proven good)
- âœ… Full feature richness
- âœ… Best prediction quality

**Cons**:
- â³ 4-8 hours development
- â³ Complex feature engineering
- â³ Need to test all indicators
- âš ï¸ Higher computational cost

---

## ğŸ’¡ IMMEDIATE RECOMMENDATION

**Do Option A NOW + Option B LATER**

1. **Immediate (15 min)**: Disable LightGBM to stop errors
2. **This week (2-4 hours)**: Retrain with 17 features
3. **Future (optional)**: Expand to 49 features if performance warrants it

**Rationale**:
- System is already trading on fallback signals (working fine)
- XGBoost may still provide value
- Quick fix stops error spam and clarifies logs
- Can retrain properly when time permits

---

## ğŸ“ IMPLEMENTATION PLAN

### Step 1: Disable LightGBM (NOW)

**File**: `ai_engine/services/ensemble_predictor_service.py`

Find the agent initialization section and comment out LGBM:
```python
# Initialize agents
xgb_agent = XGBoostAgent()
# lgbm_agent = LGBMAgent()  # DISABLED: Feature mismatch (needs 49, we have 17)

agents = {
    "xgboost": xgb_agent,
    # "lgbm": lgbm_agent,  # DISABLED
}
```

**Restart**:
```bash
systemctl restart quantum-ensemble-predictor
```

**Verify**:
```bash
journalctl -u quantum-ensemble-predictor -f
# Should see no more "X has 5 features but StandardScaler is expecting..." errors
```

---

### Step 2: Retrain LightGBM (LATER)

**Training Script**:
```python
# train_lgbm_17features.py
import pandas as pd
from sklearn.preprocessing import StandardScaler
import lightgbm as lgb
import pickle

# Features available in production
FEATURES = [
    'price_return_1', 'price_return_5', 'price_volatility_10',
    'price_change', 'ma_10', 'ma_20', 'ma_50', 'ma_cross_10_20',
    'rsi_14', 'macd', 'volume', 'volume_ratio',
    'bb_upper', 'bb_lower', 'bb_position', 'momentum_10'
]

# Load training data
data = load_training_data()  # From Redis/database
X = data[FEATURES]
y = data['target']  # Returns or signal

# Train scaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train LightGBM
model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.05)
model.fit(X_scaled, y)

# Save
pickle.dump(model, open('lightgbm_v20260216_v2.pkl', 'wb'))
pickle.dump(scaler, open('lightgbm_scaler_v20260216_v2.pkl', 'wb'))
```

---

## âœ… SUCCESS CRITERIA

### After Disabling LightGBM:
```
âœ… No more StandardScaler errors in logs
âœ… Ensemble predictor producing predictions
âœ… Trading continues normally
âœ… XGBoost predictions visible in signals
âœ… Error rate drops from 116/min to 0
```

### After Retraining:
```
âœ… LightGBM agent loads successfully
âœ… Predictions generated for all symbols
âœ… Model votes show "lgbm" instead of "fallback"
âœ… Confidence scores vary (not stuck at 0.72/0.68)
âœ… Trading performance improves (measure over 1 week)
```

---

## ğŸ“Š CURRENT SYSTEM STATUS

**Trading**: âœ… Working (fallback signals)  
**LightGBM**: âŒ Broken (49vs5 feature mismatch)  
**XGBoost**: â“ Unknown (not logging errors, may work)  
**Ensemble**: âš ï¸ Degraded (using fallback only)  

**Impact**: Low (trading works, just without ML intelligence)  
**Urgency**: Medium (should fix to restore ML capabilities)  
**Complexity**: Low (Option A) to High (Option C)

---

**Report End**
