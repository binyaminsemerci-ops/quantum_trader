"""
TRAIN TFT WITH QUANTILE LOSS - ROBUSTNESS OPTIMIZED
Implements best practices for crypto trading:
- Quantile loss for asymmetric returns
- Increased sequence length (120)
- Better dropout regularization
- Validation gating before save
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple
from datetime import datetime
import json
import logging

from ai_engine.tft_model import (
    TemporalFusionTransformer,
    TFTTrainer,
    QuantileLoss,
    save_model
)
from ai_engine.feature_engineer import compute_all_indicators

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class QuantileTFTTrainer(TFTTrainer):
    """
    Enhanced TFT Trainer with Quantile loss optimization
    """
    
    def __init__(self, model: TemporalFusionTransformer, device: str = 'cpu'):
        super().__init__(model, device)
        
        # ‚≠ê QUANTILE LOSS (best for crypto)
        self.quantile_loss = QuantileLoss(quantiles=[0.1, 0.5, 0.9])
        
        # Classification loss (cross-entropy)
        self.classification_loss = nn.CrossEntropyLoss()
        
        # ‚≠ê INCREASED quantile weight (more focus on distribution)
        self.quantile_weight = 0.5  # Up from 0.3 (better calibration)
        
        logger.info("üéØ Quantile TFT Trainer initialized")
        logger.info(f"   Quantile weight: {self.quantile_weight}")
        logger.info(f"   Device: {device}")
    
    def train_epoch(
        self,
        train_loader: torch.utils.data.DataLoader,
        optimizer: torch.optim.Optimizer
    ) -> float:
        """Train one epoch with quantile-focused loss"""
        self.model.train()
        total_loss = 0
        total_class_loss = 0
        total_quant_loss = 0
        num_batches = len(train_loader)
        
        for batch_idx, (sequences, targets) in enumerate(train_loader):
            sequences = sequences.to(self.device)
            targets = targets.to(self.device)
            
            # Forward pass
            optimizer.zero_grad()
            logits, quantiles, _ = self.model(sequences)
            
            # ‚≠ê QUANTILE-FOCUSED LOSS
            class_loss = self.classification_loss(logits, targets.long())
            quant_loss = self.quantile_loss(quantiles, targets.float())
            
            # Combined loss (30% quantile vs 10% before)
            loss = class_loss + self.quantile_weight * quant_loss
            
            # Backward pass
            loss.backward()
            
            # ‚≠ê GRADIENT CLIPPING (prevents exploding gradients)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            total_loss += loss.item()
            total_class_loss += class_loss.item()
            total_quant_loss += quant_loss.item()
            
            # Progress
            if batch_idx % 50 == 0:
                logger.info(
                    f"   Batch {batch_idx}/{num_batches}: "
                    f"Loss={loss.item():.4f} "
                    f"(Class={class_loss.item():.4f}, Quant={quant_loss.item():.4f})"
                )
        
        avg_loss = total_loss / num_batches
        avg_class = total_class_loss / num_batches
        avg_quant = total_quant_loss / num_batches
        
        logger.info(f"‚úÖ Epoch complete: Loss={avg_loss:.4f} (Class={avg_class:.4f}, Quant={avg_quant:.4f})")
        
        return avg_loss
    
    def evaluate(
        self,
        val_loader: torch.utils.data.DataLoader
    ) -> Dict[str, float]:
        """Evaluate with detailed quantile metrics"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        # Quantile calibration metrics
        quantile_errors = {0.1: [], 0.5: [], 0.9: []}
        
        with torch.no_grad():
            for sequences, targets in val_loader:
                sequences = sequences.to(self.device)
                targets = targets.to(self.device)
                
                logits, quantiles, _ = self.model(sequences)
                
                # Loss
                class_loss = self.classification_loss(logits, targets.long())
                quant_loss = self.quantile_loss(quantiles, targets.float())
                loss = class_loss + self.quantile_weight * quant_loss
                
                total_loss += loss.item()
                
                # Accuracy
                predictions = torch.argmax(logits, dim=1)
                correct += (predictions == targets).sum().item()
                total += targets.size(0)
                
                # ‚≠ê QUANTILE CALIBRATION
                for i, q in enumerate([0.1, 0.5, 0.9]):
                    errors = (targets.float() - quantiles[:, i]).cpu().numpy()
                    quantile_errors[q].extend(errors)
        
        accuracy = correct / total * 100
        avg_loss = total_loss / len(val_loader)
        
        # ‚≠ê ASYMMETRIC RISK/REWARD METRICS
        q10_coverage = np.mean([e > 0 for e in quantile_errors[0.1]])  # Should be ~10%
        q90_coverage = np.mean([e < 0 for e in quantile_errors[0.9]])  # Should be ~10%
        
        return {
            'loss': avg_loss,
            'accuracy': accuracy,
            'q10_coverage': q10_coverage * 100,  # % below P10
            'q90_coverage': q90_coverage * 100,  # % above P90
            'quantile_calibration': abs(q10_coverage - 0.1) + abs(q90_coverage - 0.1)
        }


def load_training_data(
    data_path: str = "data/binance_training_data.csv",
    sequence_length: int = 120
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Load and prepare training data for TFT
    
    Returns:
        X_train, X_val, y_train, y_val
    """
    logger.info(f"üìÇ Loading training data from {data_path}")
    
    df = pd.read_csv(data_path)
    logger.info(f"   Loaded {len(df)} rows")
    
    # Group by symbol
    symbols = df['symbol'].unique()
    logger.info(f"   Found {len(symbols)} symbols")
    
    sequences = []
    targets = []
    
    for symbol in symbols:
        symbol_df = df[df['symbol'] == symbol].sort_values('timestamp')
        
        if len(symbol_df) < sequence_length + 1:
            continue
        
        # Compute indicators
        symbol_df = compute_all_indicators(symbol_df, use_advanced=False)
        
        # Extract features (14 base features)
        feature_cols = [
            'Close', 'Volume', 'EMA_10', 'EMA_50', 'RSI_14',
            'MACD', 'MACD_signal', 'BB_upper', 'BB_middle', 'BB_lower',
            'ATR', 'volume_sma_20', 'price_change_pct', 'high_low_range'
        ]
        
        features = symbol_df[feature_cols].values
        
        # Create sequences
        for i in range(len(features) - sequence_length):
            seq = features[i:i+sequence_length]
            target_idx = i + sequence_length
            
            # Target: price movement direction
            if target_idx < len(symbol_df):
                future_close = symbol_df.iloc[target_idx]['Close']
                current_close = symbol_df.iloc[target_idx - 1]['Close']
                pct_change = (future_close - current_close) / current_close
                
                # Classification: 0=SELL (<-1%), 1=HOLD (-1% to 1%), 2=BUY (>1%)
                if pct_change < -0.01:
                    target = 0  # SELL
                elif pct_change > 0.01:
                    target = 2  # BUY
                else:
                    target = 1  # HOLD
                
                sequences.append(seq)
                targets.append(target)
    
    logger.info(f"‚úÖ Created {len(sequences)} sequences")
    
    # Convert to tensors
    X = torch.tensor(np.array(sequences), dtype=torch.float32)
    y = torch.tensor(np.array(targets), dtype=torch.long)
    
    # ‚≠ê NORMALIZATION (per feature)
    mean = X.mean(dim=(0, 1), keepdim=True)
    std = X.std(dim=(0, 1), keepdim=True)
    X = (X - mean) / (std + 1e-8)
    
    # Extract normalization stats for model saving
    feature_mean = mean.squeeze().numpy().astype(np.float32)
    feature_std = std.squeeze().numpy().astype(np.float32)
    
    # Save normalization stats (JSON for reference)
    stats = {
        'mean': feature_mean.tolist(),
        'std': feature_std.tolist()
    }
    
    stats_path = Path("ai_engine/models/tft_normalization.json")
    stats_path.parent.mkdir(exist_ok=True, parents=True)
    with open(stats_path, 'w') as f:
        json.dump(stats, f, indent=2)
    logger.info(f"‚úÖ Normalization stats saved to {stats_path}")
    
    # ‚≠ê TRAIN/VAL SPLIT (80/20)
    n = len(X)
    split_idx = int(n * 0.8)
    
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]
    
    logger.info(f"üìä Train: {len(X_train)} | Val: {len(X_val)}")
    
    return X_train, X_val, y_train, y_val, feature_mean, feature_std


def main():
    """Train TFT with quantile loss optimization"""
    
    print("\n" + "="*60)
    print("üöÄ TFT TRAINING WITH QUANTILE LOSS (ROBUSTNESS OPTIMIZED)")
    print("="*60 + "\n")
    
    # Device
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"üñ•Ô∏è  Device: {device}")
    
    # ‚≠ê ENHANCED MODEL CONFIGURATION
    model = TemporalFusionTransformer(
        input_size=14,
        sequence_length=120,  # ‚≠ê INCREASED from 60
        hidden_size=128,      # ‚≠ê Optimal for crypto
        num_heads=8,
        num_layers=3,
        dropout=0.2,          # ‚≠ê INCREASED from 0.1
        num_classes=3
    )
    
    model.to(device)
    logger.info(f"‚úÖ Model initialized with {sum(p.numel() for p in model.parameters())} parameters")
    
    # Load data
    X_train, X_val, y_train, y_val, feature_mean, feature_std = load_training_data(sequence_length=120)
    
    # DataLoaders
    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)
    val_dataset = torch.utils.data.TensorDataset(X_val, y_val)
    
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=64, shuffle=True, num_workers=0
    )
    val_loader = torch.utils.data.DataLoader(
        val_dataset, batch_size=64, shuffle=False, num_workers=0
    )
    
    # ‚≠ê QUANTILE TRAINER
    trainer = QuantileTFTTrainer(model, device=device)
    
    # ‚≠ê OPTIMIZER with weight decay for regularization
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=1e-3,
        weight_decay=1e-4  # ‚≠ê L2 regularization
    )
    
    # ‚≠ê LEARNING RATE SCHEDULER (reduce on plateau)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5
    )
    
    # Training loop
    num_epochs = 50
    best_val_loss = float('inf')
    patience = 10
    patience_counter = 0
    
    logger.info(f"\nüèãÔ∏è Starting training for {num_epochs} epochs\n")
    
    for epoch in range(num_epochs):
        logger.info(f"üìà Epoch {epoch+1}/{num_epochs}")
        
        # Train
        train_loss = trainer.train_epoch(train_loader, optimizer)
        
        # Validate
        val_metrics = trainer.evaluate(val_loader)
        val_loss = val_metrics['loss']
        
        logger.info(f"üìä Validation metrics:")
        logger.info(f"   Loss: {val_loss:.4f}")
        logger.info(f"   Accuracy: {val_metrics['accuracy']:.2f}%")
        logger.info(f"   Q10 coverage: {val_metrics['q10_coverage']:.1f}% (target: 10%)")
        logger.info(f"   Q90 coverage: {val_metrics['q90_coverage']:.1f}% (target: 10%)")
        logger.info(f"   Quantile calibration error: {val_metrics['quantile_calibration']:.4f}")
        
        # LR scheduling
        scheduler.step(val_loss)
        
        # ‚≠ê SAVE BEST MODEL (validation gating)
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            
            model_path = Path("ai_engine/models/tft_model.pth")
            model_path.parent.mkdir(exist_ok=True, parents=True)
            save_model(model, str(model_path), feature_mean=feature_mean, feature_std=feature_std)
            
            logger.info(f"‚úÖ Best model saved (val_loss={val_loss:.4f})")
            
            # Save metadata
            metadata = {
                'timestamp': datetime.now().isoformat(),
                'epoch': epoch + 1,
                'train_loss': train_loss,
                'val_loss': val_loss,
                'accuracy': val_metrics['accuracy'],
                'q10_coverage': val_metrics['q10_coverage'],
                'q90_coverage': val_metrics['q90_coverage'],
                'quantile_calibration': val_metrics['quantile_calibration'],
                'sequence_length': 120,
                'hidden_size': 128,
                'dropout': 0.2,
                'quantile_weight': 0.5
            }
            
            metadata_path = Path("ai_engine/models/tft_metadata.json")
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
        
        else:
            patience_counter += 1
            logger.info(f"‚è∏Ô∏è  No improvement ({patience_counter}/{patience})")
            
            if patience_counter >= patience:
                logger.info(f"üõë Early stopping triggered")
                break
        
        logger.info("")
    
    print("\n" + "="*60)
    print("‚úÖ TRAINING COMPLETE!")
    print("="*60)
    print(f"Best validation loss: {best_val_loss:.4f}")
    print(f"Model saved to: ai_engine/models/tft_model.pth")
    print("\nüöÄ Ready to deploy!")
    print("   Restart backend to load new model")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()
