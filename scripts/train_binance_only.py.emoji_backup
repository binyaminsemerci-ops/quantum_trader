"""
Binance-Only Training Script
Trains XGBoost and TFT models using ONLY Binance data (no CoinGecko).

This bypasses CoinGecko rate limits and trains on pure OHLCV + technical indicators.
"""
import asyncio
import sys
import os
from pathlib import Path
from datetime import datetime, timezone
from typing import List, Dict, Any
import pandas as pd
import numpy as np

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from ai_engine.train_and_save import logger
from binance.client import Client as BinanceClient
from config.config import load_config
import pickle
import xgboost as xgb
from sklearn.preprocessing import StandardScaler


def calculate_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """Calculate technical indicators from OHLCV data."""
    try:
        # Ensure numeric types
        for col in ['open', 'high', 'low', 'close', 'volume']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # Price changes
        df['price_change'] = df['close'].pct_change()
        df['high_low_range'] = (df['high'] - df['low']) / df['close']
        
        # Volume indicators
        df['volume_change'] = df['volume'].pct_change()
        df['volume_ma_ratio'] = df['volume'] / df['volume'].rolling(20).mean()
        
        # Moving averages
        df['ema_10'] = df['close'].ewm(span=10, adjust=False).mean()
        df['ema_20'] = df['close'].ewm(span=20, adjust=False).mean()
        df['ema_50'] = df['close'].ewm(span=50, adjust=False).mean()
        
        # EMA crosses and divergences
        df['ema_10_20_cross'] = (df['ema_10'] - df['ema_20']) / df['close']
        df['ema_10_50_cross'] = (df['ema_10'] - df['ema_50']) / df['close']
        
        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi_14'] = 100 - (100 / (1 + rs))
        
        # Volatility
        df['volatility_20'] = df['close'].pct_change().rolling(20).std()
        
        # MACD
        ema_12 = df['close'].ewm(span=12, adjust=False).mean()
        ema_26 = df['close'].ewm(span=26, adjust=False).mean()
        df['macd'] = ema_12 - ema_26
        df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']
        
        # Bollinger Bands
        bb_period = 20
        df['bb_middle'] = df['close'].rolling(bb_period).mean()
        bb_std = df['close'].rolling(bb_period).std()
        df['bb_upper'] = df['bb_middle'] + (2 * bb_std)
        df['bb_lower'] = df['bb_middle'] - (2 * bb_std)
        df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
        
        # Momentum
        df['momentum_10'] = df['close'] - df['close'].shift(10)
        df['momentum_20'] = df['close'] - df['close'].shift(20)
        
        # Drop NaN rows
        df = df.dropna()
        
        return df
        
    except Exception as e:
        logger.error(f"Failed to calculate indicators: {e}")
        return df


def create_training_labels(df: pd.DataFrame, forward_periods: int = 5) -> pd.DataFrame:
    """
    Create training labels based on future price movement.
    
    Label logic:
    - 1 (BUY): Price increases > 0.5% in next N periods
    - -1 (SELL): Price decreases > 0.5% in next N periods
    - 0 (HOLD): Price changes < 0.5% in next N periods
    """
    df = df.copy()
    
    # Calculate future returns
    df['future_return'] = df['close'].shift(-forward_periods) / df['close'] - 1
    
    # Create labels
    threshold = 0.005  # 0.5% threshold
    df['label'] = 0  # Default HOLD
    df.loc[df['future_return'] > threshold, 'label'] = 1  # BUY
    df.loc[df['future_return'] < -threshold, 'label'] = -1  # SELL
    
    # Drop rows without future data
    df = df.dropna(subset=['future_return'])
    
    return df


def fetch_training_data(symbols: List[str], limit: int = 500) -> Dict[str, pd.DataFrame]:
    """Fetch OHLCV data from Binance for all symbols."""
    # Initialize Binance client
    cfg = load_config()
    client = BinanceClient(cfg.binance_api_key, cfg.binance_api_secret)
    
    data = {}
    
    logger.info(f"Fetching data for {len(symbols)} symbols...")
    
    for symbol in symbols:
        try:
            # Get OHLCV data from Binance (1h candles)
            klines = client.get_historical_klines(
                symbol=symbol,
                interval=BinanceClient.KLINE_INTERVAL_1HOUR,
                limit=limit
            )
            
            if not klines:
                logger.warning(f"No data for {symbol}")
                continue
            
            # Convert to DataFrame
            df = pd.DataFrame(klines, columns=[
                'timestamp', 'open', 'high', 'low', 'close', 'volume',
                'close_time', 'quote_volume', 'trades', 'taker_buy_base',
                'taker_buy_quote', 'ignore'
            ])
            
            # Convert timestamp to datetime
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df.set_index('timestamp', inplace=True)
            
            # Calculate indicators
            df = calculate_technical_indicators(df)
            
            # Create labels
            df = create_training_labels(df)
            
            if len(df) > 50:  # Minimum samples required
                data[symbol] = df
                logger.info(f"‚úÖ {symbol}: {len(df)} samples")
            else:
                logger.warning(f"‚ö†Ô∏è {symbol}: Only {len(df)} samples (need 50+)")
                
        except Exception as e:
            logger.error(f"‚ùå {symbol}: {e}")
            continue
    
    return data


def prepare_features(df: pd.DataFrame) -> pd.DataFrame:
    """Select and prepare features for training."""
    feature_columns = [
        'price_change', 'high_low_range', 'volume_change', 'volume_ma_ratio',
        'ema_10', 'ema_20', 'ema_50', 'ema_10_20_cross', 'ema_10_50_cross',
        'rsi_14', 'volatility_20', 'macd', 'macd_signal', 'macd_hist',
        'bb_position', 'momentum_10', 'momentum_20'
    ]
    
    # Select only available features
    available_features = [col for col in feature_columns if col in df.columns]
    
    return df[available_features]


def train_xgboost_model(data: Dict[str, pd.DataFrame]) -> tuple:
    """Train XGBoost model on combined data."""
    logger.info("Training XGBoost model...")
    
    # Combine all symbol data
    all_features = []
    all_labels = []
    
    for symbol, df in data.items():
        features = prepare_features(df)
        labels = df['label']
        
        all_features.append(features)
        all_labels.append(labels)
    
    # Concatenate all data
    X = pd.concat(all_features, ignore_index=True)
    y = pd.concat(all_labels, ignore_index=True)
    
    logger.info(f"Training on {len(X)} samples with {X.shape[1]} features")
    logger.info(f"Label distribution: BUY={sum(y==1)}, SELL={sum(y==-1)}, HOLD={sum(y==0)}")
    
    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Train XGBoost
    # Convert labels: -1,0,1 -> 0,1,2 for multiclass
    y_multiclass = y + 1
    
    params = {
        'objective': 'multi:softmax',
        'num_class': 3,
        'max_depth': 6,
        'learning_rate': 0.1,
        'n_estimators': 200,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'random_state': 42,
        'n_jobs': -1
    }
    
    model = xgb.XGBClassifier(**params)
    model.fit(X_scaled, y_multiclass)
    
    # Feature importance
    importance = pd.DataFrame({
        'feature': X.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    logger.info("Top 10 features:")
    for _, row in importance.head(10).iterrows():
        logger.info(f"  {row['feature']}: {row['importance']:.4f}")
    
    return model, scaler, X.columns.tolist()


def save_models(model, scaler, feature_names: List[str]):
    """Save trained models to disk."""
    models_dir = Path(__file__).parent.parent / "ai_engine" / "models"
    models_dir.mkdir(parents=True, exist_ok=True)
    
    # Save XGBoost model
    model_path = models_dir / "xgb_model.pkl"
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    logger.info(f"‚úÖ Saved XGBoost model: {model_path}")
    
    # Save scaler
    scaler_path = models_dir / "scaler.pkl"
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    logger.info(f"‚úÖ Saved scaler: {scaler_path}")
    
    # Save metadata
    metadata = {
        'trained_at': datetime.now(timezone.utc).isoformat(),
        'feature_names': feature_names,
        'data_source': 'binance_only',
        'model_type': 'xgboost_multiclass'
    }
    
    metadata_path = models_dir / "metadata.json"
    import json
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    logger.info(f"‚úÖ Saved metadata: {metadata_path}")


def main():
    """Main training pipeline."""
    logger.info("=" * 60)
    logger.info("BINANCE-ONLY TRAINING PIPELINE")
    logger.info("=" * 60)
    
    # Try to load from CSV first (full 136K dataset)
    csv_path = Path("data/binance_training_data_full.csv")
    if csv_path.exists():
        logger.info(f"üìÇ Loading training data from {csv_path}")
        df = pd.read_csv(csv_path)
        logger.info(f"‚úÖ Loaded {len(df)} rows from CSV")
        
        # Group by symbol and process each
        data = {}
        for symbol in df['symbol'].unique():
            symbol_df = df[df['symbol'] == symbol].copy()
            
            # Ensure proper column names (CSV has lowercase)
            if 'close' in symbol_df.columns and 'Close' not in symbol_df.columns:
                symbol_df.rename(columns={
                    'open': 'Open', 'high': 'High', 'low': 'Low',
                    'close': 'Close', 'volume': 'Volume'
                }, inplace=True)
            
            # Calculate technical indicators
            symbol_df = calculate_technical_indicators(symbol_df)
            
            # Create training labels
            symbol_df = create_training_labels(symbol_df)
            
            # Drop NaN rows
            symbol_df = symbol_df.dropna()
            
            if len(symbol_df) > 50:  # Minimum rows for training
                data[symbol] = symbol_df
        
        logger.info(f"‚úÖ Collected data for {len(data)} symbols from CSV")
    else:
        logger.info("üì° CSV not found, fetching from Binance API...")
        # Top 15 liquid symbols (avoid rate limits)
        symbols = [
            'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT', 'XRPUSDT',
            'DOGEUSDT', 'ADAUSDT', 'AVAXUSDT', 'DOTUSDT', 'LINKUSDT',
            'MATICUSDT', 'UNIUSDT', 'LTCUSDT', 'NEARUSDT', 'ATOMUSDT'
        ]
        
        logger.info(f"Training on {len(symbols)} symbols")
        logger.info(f"Symbols: {', '.join(symbols)}")
        
        # Fetch data (sync function - using python-binance directly)
        data = fetch_training_data(symbols, limit=500)
        
        if not data:
            logger.error("‚ùå No data fetched! Cannot train.")
            return
        
        logger.info(f"‚úÖ Collected data for {len(data)} symbols")
    
    # Train XGBoost
    model, scaler, feature_names = train_xgboost_model(data)
    
    # Save models
    save_models(model, scaler, feature_names)
    
    logger.info("=" * 60)
    logger.info("‚úÖ TRAINING COMPLETE!")
    logger.info("=" * 60)
    logger.info("Next steps:")
    logger.info("1. Restart backend: docker-compose restart backend")
    logger.info("2. Monitor signals: python monitor_hybrid.py -i 5")
    logger.info("3. Check confidence levels in logs")


if __name__ == "__main__":
    # Now fully synchronous
    main()
