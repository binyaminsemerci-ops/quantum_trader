"""
Model Supervisor & Predictive Governance
Phase 4D + 4E Implementation

Self-regulating ensemble controller that:
- Monitors real-time performance (MAPE, PnL)
- Detects model drift > threshold
- Auto-adjusts ensemble weights
- Triggers automatic retraining when precision drops
"""

import logging
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)


class ModelSupervisorGovernance:
    """
    Autonomous model supervisor with predictive governance.
    
    Responsibilities:
    1. Track performance metrics per model (MAPE, PnL)
    2. Detect drift and performance degradation
    3. Dynamically adjust ensemble weights
    4. Trigger retraining when needed
    """
    
    def __init__(
        self,
        drift_threshold: float = 0.06,
        retrain_interval: int = 7200,
        smooth: float = 0.3,
        min_samples: int = 5
    ):
        """
        Initialize the Model Supervisor & Governance system.
        
        Args:
            drift_threshold: MAPE threshold to trigger drift detection (default: 6%)
            retrain_interval: Minimum seconds between retraining cycles (default: 2h)
            smooth: Smoothing factor for weight adjustment (0-1, default: 0.3)
            min_samples: Minimum samples required for drift detection
        """
        self.models: Dict[str, Any] = {}
        self.metrics: Dict[str, Dict] = {}
        self.drift_threshold = drift_threshold
        self.retrain_interval = retrain_interval
        self.smooth = smooth
        self.min_samples = min_samples
        self.last_retrain = datetime.utcnow()
        
        logger.info(
            f"[Supervisor] Initialized - Drift threshold: {drift_threshold:.1%}, "
            f"Retrain interval: {retrain_interval}s, Smoothing: {smooth}"
        )

    def register(self, name: str, model: Any) -> None:
        """
        Register a model for supervision.
        
        Args:
            name: Model identifier
            model: Model instance
        """
        self.models[name] = model
        self.metrics[name] = {
            "mape": [],
            "pnl": [],
            "weight": 1.0 / max(len(self.models), 1),  # Equal initial weights
            "drift_count": 0,
            "retrain_count": 0,
            "last_mape": 0.0
        }
        logger.info(f"[Supervisor] âœ… Registered model: {name}")

    def update_metrics(
        self,
        predictions: Dict[str, np.ndarray],
        actuals: np.ndarray,
        pnl: Dict[str, float]
    ) -> None:
        """
        Update performance metrics for all models.
        
        Args:
            predictions: Dictionary of model predictions {model_name: predictions}
            actuals: Actual observed values
            pnl: PnL per model {model_name: pnl_value}
        """
        for model_name, pred in predictions.items():
            if model_name not in self.metrics:
                logger.warning(f"[Supervisor] Unknown model: {model_name}")
                continue
                
            try:
                # Convert to numpy arrays
                actual_array = np.array(actuals).flatten()
                pred_array = np.array(pred).flatten()
                
                # Calculate MAPE
                mape = np.mean(np.abs((actual_array - pred_array) / (actual_array + 1e-8)))
                mape = float(np.clip(mape, 0, 1))  # Clip to [0, 1]
                
                # Update metrics
                self.metrics[model_name]["mape"].append(mape)
                self.metrics[model_name]["pnl"].append(float(pnl.get(model_name, 0)))
                self.metrics[model_name]["last_mape"] = mape
                
                # Keep only last 100 samples
                for key in ("mape", "pnl"):
                    if len(self.metrics[model_name][key]) > 100:
                        self.metrics[model_name][key].pop(0)
                        
            except Exception as e:
                logger.error(f"[Supervisor] Metric update error for {model_name}: {e}")

    def detect_drift(self) -> List[str]:
        """
        Detect models with significant drift.
        
        Returns:
            List of model names with detected drift
        """
        drifted = []
        
        for model_name, data in self.metrics.items():
            if len(data["mape"]) < self.min_samples:
                continue
                
            # Calculate rolling average MAPE
            avg_mape = np.mean(data["mape"][-10:])  # Last 10 predictions
            
            # Check drift threshold
            if avg_mape > self.drift_threshold:
                drifted.append(model_name)
                data["drift_count"] += 1
                logger.warning(
                    f"[Supervisor] ðŸš¨ Drift detected in {model_name} - "
                    f"MAPE: {avg_mape:.3f} (threshold: {self.drift_threshold:.3f})"
                )
                
        return drifted

    def retrain(self, model_name: str) -> bool:
        """
        Retrain a specific model.
        
        Args:
            model_name: Name of the model to retrain
            
        Returns:
            Success status
        """
        try:
            if model_name not in self.models:
                logger.error(f"[Supervisor] Cannot retrain unknown model: {model_name}")
                return False
                
            # Note: In a real implementation, this would trigger actual retraining
            # For now, we log the event and reset metrics
            logger.info(f"[Supervisor] ðŸ”„ Initiating retraining for {model_name}")
            
            # Reset drift-related metrics
            self.metrics[model_name]["mape"] = []
            self.metrics[model_name]["drift_count"] = 0
            self.metrics[model_name]["retrain_count"] += 1
            
            logger.info(f"[Supervisor] âœ… Retrained {model_name} successfully")
            return True
            
        except Exception as e:
            logger.error(f"[Supervisor] âŒ Retrain failed for {model_name}: {e}")
            return False

    def adjust_weights(self) -> Dict[str, float]:
        """
        Dynamically adjust ensemble weights based on recent performance.
        
        Uses a combination of PnL and inverse MAPE for scoring.
        
        Returns:
            Dictionary of normalized weights {model_name: weight}
        """
        total_score = 0
        scores = {}
        
        # Calculate performance scores
        for model_name, data in self.metrics.items():
            if len(data["pnl"]) < 2 or len(data["mape"]) < 2:
                # Insufficient data, use current weight
                scores[model_name] = data["weight"]
                continue
                
            # Average recent PnL (last 10 samples)
            avg_pnl = np.mean(data["pnl"][-10:])
            
            # Average recent MAPE (lower is better)
            avg_mape = np.mean(data["mape"][-10:])
            
            # Score: PnL / MAPE (higher is better)
            # Add small epsilon to avoid division by zero
            score = max(0.01, avg_pnl / (avg_mape + 1e-8))
            
            # Apply exponential smoothing
            new_weight = self.smooth * score + (1 - self.smooth) * data["weight"]
            scores[model_name] = max(0.01, new_weight)  # Minimum weight
            total_score += scores[model_name]
        
        # Normalize weights to sum to 1.0
        if total_score > 0:
            for model_name in scores:
                scores[model_name] /= total_score
                self.metrics[model_name]["weight"] = scores[model_name]
        
        # Log weight distribution
        weight_str = ", ".join(f"{n}={w:.2f}" for n, w in scores.items())
        logger.info(f"[Governance] ðŸ“Š Adjusted weights: {weight_str}")
        
        return scores

    def run_cycle(
        self,
        predictions: Dict[str, np.ndarray],
        actuals: np.ndarray,
        pnl: Dict[str, float]
    ) -> Dict[str, float]:
        """
        Execute a complete supervision cycle.
        
        Steps:
        1. Update metrics with latest predictions
        2. Detect drift
        3. Trigger retraining if needed
        4. Adjust ensemble weights
        
        Args:
            predictions: Model predictions {model_name: predictions}
            actuals: Actual values
            pnl: PnL per model {model_name: pnl}
            
        Returns:
            Updated ensemble weights {model_name: weight}
        """
        # Step 1: Update metrics
        self.update_metrics(predictions, actuals, pnl)
        
        # Step 2: Detect drift
        drifted_models = self.detect_drift()
        
        # Step 3: Check if retraining is needed
        now = datetime.utcnow()
        time_since_retrain = (now - self.last_retrain).total_seconds()
        should_retrain = (
            len(drifted_models) > 0 or
            time_since_retrain > self.retrain_interval
        )
        
        if should_retrain:
            logger.info(
                f"[Supervisor] ðŸ”„ Retraining cycle triggered - "
                f"Drifted: {len(drifted_models)}, "
                f"Time since last: {time_since_retrain:.0f}s"
            )
            
            # Retrain drifted models
            for model_name in drifted_models:
                self.retrain(model_name)
                
            self.last_retrain = now
        
        # Step 4: Adjust weights
        weights = self.adjust_weights()
        
        return weights

    def get_status(self) -> Dict[str, Any]:
        """
        Get current supervisor status and metrics.
        
        Returns:
            Dictionary with status information
        """
        status = {
            "active_models": len(self.models),
            "drift_threshold": self.drift_threshold,
            "retrain_interval": self.retrain_interval,
            "last_retrain": self.last_retrain.isoformat(),
            "models": {}
        }
        
        for model_name, data in self.metrics.items():
            status["models"][model_name] = {
                "weight": round(data["weight"], 4),
                "last_mape": round(data["last_mape"], 4),
                "avg_pnl": round(np.mean(data["pnl"][-10:]), 4) if data["pnl"] else 0,
                "drift_count": data["drift_count"],
                "retrain_count": data["retrain_count"],
                "samples": len(data["mape"])
            }
        
        return status
