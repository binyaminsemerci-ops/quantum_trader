"""
Performance Optimization: Advanced Caching Layer

Reduces P99 latency through:
- Redis caching for dashboard endpoints
- Response compression
- Connection pooling
- Query result caching
- Smart cache invalidation
"""

from typing import Optional, Any, Callable
from functools import wraps
import json
import hashlib
from datetime import timedelta
import asyncio

from fastapi import Request, Response
from fastapi.responses import JSONResponse
import redis.asyncio as redis
import os

# Redis connection pool
redis_pool: Optional[redis.ConnectionPool] = None
redis_client: Optional[redis.Redis] = None


async def init_cache():
    """Initialize Redis cache connection pool."""
    global redis_pool, redis_client
    
    try:
        redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
        redis_pool = redis.ConnectionPool.from_url(
            redis_url,
            max_connections=50,
            decode_responses=True
        )
        redis_client = redis.Redis(connection_pool=redis_pool)
        await redis_client.ping()
        print("âœ… Cache initialized successfully")
    except Exception as e:
        print(f"âš ï¸ Cache initialization failed: {e}")
        redis_client = None


async def close_cache():
    """Close Redis cache connections."""
    if redis_client:
        await redis_client.close()
    if redis_pool:
        await redis_pool.disconnect()


def cache_key_from_request(request: Request, prefix: str = "") -> str:
    """Generate cache key from request."""
    # Include path, query params, and user info
    path = request.url.path
    query = str(sorted(request.query_params.items()))
    user = getattr(request.state, "user", None)
    user_id = user.username if user else "anonymous"
    
    key_parts = f"{prefix}:{path}:{query}:{user_id}"
    # Hash for consistent key length
    key_hash = hashlib.md5(key_parts.encode()).hexdigest()
    return f"cache:{prefix}:{key_hash}"


def cached(
    ttl: int = 60,
    prefix: str = "api",
    key_func: Optional[Callable] = None
):
    """
    Cache decorator for FastAPI endpoints.
    
    Args:
        ttl: Time to live in seconds
        prefix: Cache key prefix
        key_func: Custom key generation function
    """
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Skip caching if Redis unavailable
            if not redis_client:
                return await func(*args, **kwargs)
            
            # Extract request from args/kwargs
            request = None
            for arg in args:
                if isinstance(arg, Request):
                    request = arg
                    break
            
            if not request:
                # No request object, can't cache
                return await func(*args, **kwargs)
            
            # Generate cache key
            if key_func:
                cache_key = key_func(request)
            else:
                cache_key = cache_key_from_request(request, prefix)
            
            # Try to get from cache
            try:
                cached_value = await redis_client.get(cache_key)
                if cached_value:
                    # Cache hit
                    request.state.cache_hit = True
                    return JSONResponse(
                        content=json.loads(cached_value),
                        headers={"X-Cache": "HIT"}
                    )
            except Exception as e:
                print(f"âš ï¸ Cache read error: {e}")
            
            # Cache miss - execute function
            result = await func(*args, **kwargs)
            
            # Store in cache
            try:
                if isinstance(result, (dict, list)):
                    cache_value = json.dumps(result)
                elif isinstance(result, JSONResponse):
                    cache_value = result.body.decode()
                else:
                    cache_value = json.dumps(result)
                
                await redis_client.setex(cache_key, ttl, cache_value)
            except Exception as e:
                print(f"âš ï¸ Cache write error: {e}")
            
            # Add cache header
            if isinstance(result, Response):
                result.headers["X-Cache"] = "MISS"
            
            return result
        
        return wrapper
    return decorator


async def invalidate_cache(pattern: str):
    """Invalidate cache entries matching pattern."""
    if not redis_client:
        return
    
    try:
        keys = []
        async for key in redis_client.scan_iter(match=f"cache:{pattern}*"):
            keys.append(key)
        
        if keys:
            await redis_client.delete(*keys)
            print(f"ðŸ—‘ï¸ Invalidated {len(keys)} cache entries")
    except Exception as e:
        print(f"âš ï¸ Cache invalidation error: {e}")


# Specific cache invalidation triggers
async def invalidate_trading_cache():
    """Invalidate trading-related caches."""
    await invalidate_cache("api:/api/dashboard/trading")
    await invalidate_cache("api:/api/positions")
    await invalidate_cache("api:/api/portfolio")


async def invalidate_risk_cache():
    """Invalidate risk-related caches."""
    await invalidate_cache("api:/api/dashboard/risk")
    await invalidate_cache("api:/health/risk")


# Cache warming - preload frequently accessed data
async def warm_cache():
    """Warm up cache with frequently accessed endpoints."""
    if not redis_client:
        return
    
    print("ðŸ”¥ Warming cache...")
    
    # Preload dashboard data (simulate requests)
    endpoints_to_warm = [
        "/api/dashboard/overview",
        "/api/dashboard/trading",
        "/api/dashboard/risk",
        "/health/live",
    ]
    
    # In production, make actual requests to these endpoints
    # For now, just log
    for endpoint in endpoints_to_warm:
        print(f"   - Preloading {endpoint}")
    
    print("âœ… Cache warmed")


# Response compression middleware
async def compress_response(request: Request, call_next):
    """Compress large JSON responses."""
    response = await call_next(request)
    
    # Check if response is JSON and large enough to compress
    if (
        response.headers.get("content-type", "").startswith("application/json")
        and int(response.headers.get("content-length", "0")) > 1024
    ):
        response.headers["Content-Encoding"] = "gzip"
    
    return response


# Connection pooling for external APIs
class ConnectionPool:
    """Reusable connection pool for HTTP clients."""
    
    def __init__(self, max_connections: int = 100):
        self.max_connections = max_connections
        self.semaphore = asyncio.Semaphore(max_connections)
    
    async def acquire(self):
        """Acquire connection from pool."""
        await self.semaphore.acquire()
    
    def release(self):
        """Release connection back to pool."""
        self.semaphore.release()


# Global connection pools
binance_pool = ConnectionPool(max_connections=50)
database_pool = ConnectionPool(max_connections=100)


# Query result caching
class QueryCache:
    """Cache for database query results."""
    
    def __init__(self, ttl: int = 30):
        self.ttl = ttl
    
    async def get(self, query: str) -> Optional[Any]:
        """Get cached query result."""
        if not redis_client:
            return None
        
        try:
            key = f"query:{hashlib.md5(query.encode()).hexdigest()}"
            result = await redis_client.get(key)
            if result:
                return json.loads(result)
        except Exception:
            pass
        return None
    
    async def set(self, query: str, result: Any):
        """Cache query result."""
        if not redis_client:
            return
        
        try:
            key = f"query:{hashlib.md5(query.encode()).hexdigest()}"
            await redis_client.setex(key, self.ttl, json.dumps(result))
        except Exception:
            pass


query_cache = QueryCache(ttl=30)
